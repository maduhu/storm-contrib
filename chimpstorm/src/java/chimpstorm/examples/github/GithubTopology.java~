package chimpstorm.examples.github;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import storm.trident.TridentState;
import storm.trident.TridentTopology;
import storm.trident.tuple.TridentTuple;
import storm.trident.operation.*;
import storm.trident.operation.builtin.*;
import storm.trident.testing.VisibleMemoryMapState;
//import storm.starter.trident.InstrumentedMemoryMapState;

import backtype.storm.Config;
import backtype.storm.LocalCluster;
import backtype.storm.StormSubmitter;
import backtype.storm.generated.InvalidTopologyException;
import backtype.storm.tuple.Fields;
import backtype.storm.tuple.Values;

import com.infochimps.storm.trident.spout.FileBlobStore;
import com.infochimps.storm.trident.spout.IBlobStore;
import com.infochimps.storm.trident.spout.OpaqueTransactionalBlobSpout;
import com.infochimps.storm.trident.spout.StartPolicy;

import com.fasterxml.jackson.core.*;
import com.fasterxml.jackson.databind.*;

import java.util.*;
import java.io.IOException;

public class GithubTopology {
  /*
   * Identity filter that logs a tuple.
   * Useful for logging every tuple in a stream.
   */
  @SuppressWarnings({ "serial", "rawtypes" })
  public static class LogTuple extends BaseFilter {
    private static final Logger LOG = LoggerFactory.getLogger(LogTuple.class);
      // just print the tuple
      @Override
      public boolean isKeep(TridentTuple tuple){
        LOG.info(tuple.toString());
        return true;
      }
  }
  public static class PrintAndPass extends BaseFunction {
    public void execute(TridentTuple tuple, TridentCollector collector){
      System.out.println(tuple);
      collector.emit(null);
    }
  }
  public static class GithubJsonExtract extends BaseFunction {
    //Jackson JSON parser
    private final static ObjectMapper jsonMapper = new ObjectMapper();
    private static final Logger LOG = LoggerFactory.getLogger(GithubJsonExtract.class);

    public void execute(TridentTuple tuple, TridentCollector collector){
      try {
        JsonNode node = jsonMapper.readTree(tuple.getString(0));
        if(! node.get("type").toString().equals("\"PushEvent\"")) return;
        List values = new ArrayList(2);
        //grab the language and the action
        values.add(node.get("repository").get("language"));
        values.add(node.get("payload").get("size"));
        collector.emit(values);
      } catch(JsonProcessingException jpEx){
        LOG.error("Error parsing JSON",jpEx);
      } catch (IOException ioEx) {
        LOG.error("IO Error",ioEx);
      }
      return;
    }
  }
 /* 
  public static class Sum extends BaseAggregator<Accumulator> {
    static class Accumulator {
      long val = 0;
    }

    public Accumulator init(Object batchId, TridentCollector collector){
      return new Accumulator();
    }

    public void aggregate(Accumulator acc, TridentTuple tuple, TridentCollector collector){
      acc.val += tuple.getLong(0);
    }

    public void complete(Accumulator acc, TridentCollector collector){
      collector.emit(new Values(acc.val));
    }
  }*/

  /*
   * Create and run the Github topology
   * The topology:
   * 1) reads the event stream from the github spout
   * 2) Extracts the langauge and action from the JSON
   * 3) Groups by language
   * 4) ... TBD ...
   */
  public static void main(String[] args) throws Exception, InvalidTopologyException {
    IBlobStore bs = new FileBlobStore("/Users/dlaw/dev/github-data/test-data");
    OpaqueTransactionalBlobSpout spout = new OpaqueTransactionalBlobSpout(bs, StartPolicy.EARLIEST, null);

    TridentTopology topology = new TridentTopology();
    topology.newStream("github-activities", spout)
        .parallelismHint(1)
        .each(new Fields("line"), new GithubJsonExtract(), new Fields("language","commits"))
//        .each(new Fields("language","commits"), new LogTuple())
        .groupBy(new Fields("language"))
        .persistentAggregate(new VisibleMemoryMapState.Factory(),
          new Count(), new Fields("commit-sum"))
        .newValuesStream()
        .each(new Fields("language","commit-sum"), new LogTuple());

//        .partitionAggregate(new Fields("commits"), new Count(), new Fields("commit-sum"))

    Config conf = new Config();
    conf.setMessageTimeoutSecs(10);
    System.out.println("Topology created");
    if (args.length == 0) {
        LocalCluster cluster = new LocalCluster();
        cluster.submitTopology("lang-counter", conf, topology.build());
    } else {
        conf.setNumWorkers(3);
        StormSubmitter.submitTopology(args[0], conf, topology.build());
    }
  }
}
